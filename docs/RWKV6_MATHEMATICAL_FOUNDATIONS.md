# RWKV-6 Mathematical Foundations for Mechanistic Interpretability

**Document version:** 1.0
**Date:** 2026-02-10
**Associated release:** PLIP-rs v1.2.0
**Implementation:** [`src/forward_rwkv6.rs`](../src/forward_rwkv6.rs)

This document presents the mathematical foundations underlying PLIP-rs's mechanistic
interpretability tools for RWKV-6, a gated-linear recurrent neural network. It covers
the core recurrence, the derivation of effective attention matrices, state knockout
semantics, numerical stability techniques, and planned state delta metrics. The goal
is to provide a self-contained reference for the formulas implemented in the codebase,
their relationship to prior work, and the novelty of this contribution.

---

## Table of Contents

- [1. RWKV-6 Architecture Overview](#1-rwkv-6-architecture-overview)
  - [1.1 Data-Dependent Token Mixing](#11-data-dependent-token-mixing)
  - [1.2 The WKV Recurrence](#12-the-wkv-recurrence)
  - [1.3 Channel-Mix MLP](#13-channel-mix-mlp)
- [2. Effective Attention for RWKV-6](#2-effective-attention-for-rwkv-6)
  - [2.1 Derivation from the WKV Recurrence](#21-derivation-from-the-wkv-recurrence)
  - [2.2 Handling Signed Weights: ReLU + L1 Normalisation](#22-handling-signed-weights-relu--l1-normalisation)
  - [2.3 Numerical Stability via Log-Space Prefix Sums](#23-numerical-stability-via-log-space-prefix-sums)
  - [2.4 Properties of the Effective Attention Matrix](#24-properties-of-the-effective-attention-matrix)
- [3. State Knockout](#3-state-knockout)
  - [3.1 Intervention Design](#31-intervention-design)
  - [3.2 Relationship to Transformer Attention Knockout](#32-relationship-to-transformer-attention-knockout)
  - [3.3 Metric: KL Divergence](#33-metric-kl-divergence)
- [4. State Steering](#4-state-steering)
- [5. State Delta Analysis (Planned)](#5-state-delta-analysis-planned)
  - [5.1 Write Strength](#51-write-strength)
  - [5.2 Persistence](#52-persistence)
  - [5.3 Channel Selectivity](#53-channel-selectivity)
  - [5.4 Cross-Validation with Knockout](#54-cross-validation-with-knockout)
- [6. Relationship to Prior Work and Novelty](#6-relationship-to-prior-work-and-novelty)
  - [6.1 Zimerman, Ali & Wolf (ICLR 2025) — Implicit Attention for Gated-Linear RNNs](#61-zimerman-ali--wolf-iclr-2025--implicit-attention-for-gated-linear-rnns)
  - [6.2 Endy et al. (ACL 2025) — Mamba Knockout](#62-endy-et-al-acl-2025--mamba-knockout)
  - [6.3 Paulo, Marshall & Belrose (AAAI 2025) — Interpretability Transfer](#63-paulo-marshall--belrose-aaai-2025--interpretability-transfer)
  - [6.4 What PLIP-rs Contributes](#64-what-plip-rs-contributes)
- [7. Notation Summary](#7-notation-summary)
- [8. References](#8-references)

---

## 1. RWKV-6 Architecture Overview

RWKV-6 (codename "Finch") is a gated-linear recurrent neural network introduced by
Peng et al. (2024). Unlike transformers, it has no attention matrix: information flows
through a fixed-size recurrent state that is updated at each timestep. The model used
in PLIP-rs is `RWKV/v6-Finch-1B6-HF` (hidden_size=2048, 24 layers, head_size=64,
32 heads, vocab_size=65536).

Each layer consists of two sub-blocks:

1. **Time-mix** (attention-like): data-dependent token mixing, WKV state recurrence,
   gated output
2. **Channel-mix** (MLP-like): data-dependent token shift, squared-ReLU activation

### 1.1 Data-Dependent Token Mixing

At each timestep $t$ and layer $l$, the time-mix block interpolates the current
hidden state $x_t$ with the previous hidden state $x_{t-1}$ via **data-dependent
mixing** (ddlerp). The mixing coefficients are generated by a low-rank network.

**Step 1: Pre-mixing.** A coarse interpolation produces the LoRA input:

$$\bar{x}_t = x_t + (x_{t-1} - x_t) \odot \mu_x$$

where $\mu_x \in \mathbb{R}^d$ is a learned mixing vector (`time_maa_x` in the code).

**Step 2: LoRA projection.** The pre-mixed signal is projected through a shared
low-rank bottleneck to produce five data-dependent correction terms:

$$[m^{(w)}_t, m^{(k)}_t, m^{(v)}_t, m^{(r)}_t, m^{(g)}_t] = \text{split}\bigl(W_2 \cdot \tanh(W_1 \cdot \bar{x}_t)\bigr)$$

where $W_1 \in \mathbb{R}^{5 d_{\text{extra}} \times d}$ and
$W_2 \in \mathbb{R}^{5 \times d \times d_{\text{extra}}}$ are low-rank adaptation
(LoRA) matrices with $d_{\text{extra}} = 32$. The $\text{split}$ operation partitions
the $5 d_{\text{extra}}$ intermediate dimensions into five groups before the
back-projection through $W_2$ (which has five separate $d \times d_{\text{extra}}$
slices). Note the absence of any sigmoid — the LoRA output is a direct linear
correction through a $\tanh$ bottleneck.

**Step 3: Component-wise mixing.** Each component $c$ combines a learned bias
$\mu^{(c)}_0$ with the data-dependent correction:

$$\tilde{x}^{(c)}_t = x_t + (x_{t-1} - x_t) \odot \bigl(\mu^{(c)}_0 + m^{(c)}_t\bigr)
\quad \text{for } c \in \{r, k, v, g, w\}$$

where $\mu^{(c)}_0 \in \mathbb{R}^d$ are per-component learned biases (`time_maa_r`,
`time_maa_k`, `time_maa_v`, `time_maa_g`, `time_maa_w` in the code). Note that the
mixing coefficient $\mu^{(c)}_0 + m^{(c)}_t$ weights the **previous** token $x_{t-1}$
(via the $(x_{t-1} - x_t)$ direction), not the current token.

The mixed inputs are then projected:

$$r_t = W_r \, \tilde{x}^{(r)}_t, \quad
  k_t = W_k \, \tilde{x}^{(k)}_t, \quad
  v_t = W_v \, \tilde{x}^{(v)}_t, \quad
  g_t = W_g \, \tilde{x}^{(g)}_t$$

All projections produce tensors of shape $[H, D]$ where $H = 32$ (heads) and
$D = 64$ (head dimension).

The time decay is data-dependent:

$$w_t = W_{w_0} + W_{w_2} \cdot \tanh\bigl(W_{w_1} \cdot \tilde{x}^{(w)}_t\bigr)$$

where $W_{w_0}$ is a learned bias, and $W_{w_1} \in \mathbb{R}^{d_{\text{decay\_extra}} \times d}$,
$W_{w_2} \in \mathbb{R}^{d \times d_{\text{decay\_extra}}}$ are LoRA matrices with
$d_{\text{decay\_extra}} = 64$. As with the mixing LoRA, there is no sigmoid — this
is a linear correction through a $\tanh$ bottleneck. This is a key difference from
RWKV-4, where the decay was a fixed learned vector.

### 1.2 The WKV Recurrence

The core of the time-mix block is the **WKV state recurrence**. Let
$d_t = \exp(-\exp(w_t)) \in (0, 1)^{H \times D}$ be the per-channel decay factor.
The recurrent state $S_t \in \mathbb{R}^{H \times D \times D}$ evolves as:

$$S_t = \text{diag}(d_t) \cdot S_{t-1} + k_t \, v_t^\top$$

where:
- $\text{diag}(d_t)$ is a diagonal matrix applying per-channel decay
- $k_t \, v_t^\top \in \mathbb{R}^{H \times D \times D}$ is the outer product
  (the "write" to the state)
- $S_0 = 0$ (zero-initialised state)

The output at timestep $t$ uses the **previous** state $S_{t-1}$ plus a bonus term
for the current timestep:

$$o_t = r_t^\top \cdot \bigl[\text{diag}(u) \cdot k_t \, v_t^\top + S_{t-1}\bigr]$$

where $u \in \mathbb{R}^{H \times D}$ (called `time_faaaa` or `time_first` in the
code) is a learned per-head bonus for the current position. After GroupNorm and gating:

$$\hat{o}_t = W_o \cdot \bigl(\text{SiLU}(g_t) \odot \text{GroupNorm}(o_t)\bigr)$$

### 1.3 Channel-Mix MLP

The channel-mix block uses data-dependent token shift (same ddlerp mechanism, but
with separate mixing coefficients) followed by:

$$\text{out}_t = \sigma(r'_t) \odot \bigl(W'_v \cdot \text{ReLU}(k'_t)^2\bigr)$$

where $\text{ReLU}(x)^2$ is the squared-ReLU activation. This replaces the SwiGLU
MLP used in transformer backends.

---

## 2. Effective Attention for RWKV-6

Transformer models produce explicit attention matrices $A \in \mathbb{R}^{T \times T}$
via softmax over $QK^\top$. RWKV-6 has no such matrix — information flows through the
recurrent state. To enable attention-based interpretability (e.g., measuring attention
from test markers to function tokens), we derive an **effective attention matrix** from
the WKV recurrence.

### 2.1 Derivation from the WKV Recurrence

Starting from the output equation at position $t$:

$$o_t = r_t^\top \cdot \bigl[\text{diag}(u) \cdot k_t \, v_t^\top + S_{t-1}\bigr]$$

We unroll the state $S_{t-1}$ using the recurrence $S_i = \text{diag}(d_i) \cdot S_{i-1} + k_i \, v_i^\top$:

$$S_{t-1} = \sum_{i=0}^{t-1} \Bigl(\prod_{j=i+1}^{t-1} \text{diag}(d_j)\Bigr) \cdot k_i \, v_i^\top$$

Substituting back:

$$o_t = r_t^\top \cdot \text{diag}(u) \cdot k_t \, v_t^\top + \sum_{i=0}^{t-1} r_t^\top \cdot \Bigl(\prod_{j=i+1}^{t-1} \text{diag}(d_j)\Bigr) \cdot k_i \, v_i^\top$$

This can be written as a weighted sum over all source positions $i \leq t$:

$$o_t = \sum_{i=0}^{t} \alpha_{\text{raw}}(t, i) \cdot v_i^\top$$

where the **raw effective attention weight** is a scalar per $(t, i, h)$ triplet,
computed as a dot product over the head-channel dimension $D$:

$$\boxed{\alpha_{\text{raw}}(t, i, h) = \sum_{d=1}^{D} r_t[h,d] \cdot k_i[h,d] \cdot \prod_{j=i+1}^{t-1} d_j[h,d] \quad \text{for } i < t}$$

$$\boxed{\alpha_{\text{raw}}(t, t, h) = \sum_{d=1}^{D} r_t[h,d] \cdot k_t[h,d] \cdot u[h,d] \quad \text{(diagonal: current position)}}$$

**Important details:**

- The decay product runs from $j = i+1$ to $j = t-1$ (not $j = t$), because the
  output uses $S_{t-1}$, which has already been decayed up to step $t-1$.
- For $i = t-1$: the product $\prod_{j=t}^{t-1}$ is empty, so the cumulative decay
  is 1 — the immediately preceding token has no decay applied.
- Both $r_t$ and $k_i$ are **signed** (no activation function constraining their sign),
  so $\alpha_{\text{raw}}$ can be negative.
- Both $d_j$ and $u$ are **vectors** of dimension $D$ per head, not scalars. The
  effective attention aggregates $D$ per-channel contributions via a dot product.

### 2.2 Handling Signed Weights: ReLU + L1 Normalisation

Since $\alpha_{\text{raw}}(t, i, h)$ can be negative (due to signed $r$ and $k$
components), standard softmax normalisation is not applicable. We apply
**ReLU followed by L1 normalisation**:

$$\alpha(t, i, h) = \frac{\max\bigl(0,\; \alpha_{\text{raw}}(t, i, h)\bigr)}{\sum_{j=0}^{t} \max\bigl(0,\; \alpha_{\text{raw}}(t, j, h)\bigr)}$$

**Rationale:**

- Negative contributions represent inhibitory interactions (source $i$ pushes the
  output *away* from $v_i$). For attention visualisation and probing experiments,
  we focus on excitatory contributions.
- L1 normalisation ensures each row sums to 1.0, producing a valid distribution
  compatible with the `AttentionCache` infrastructure shared with transformer backends.
- Alternative normalisations (softmax, abs+L1) were considered but ReLU+L1 most
  faithfully preserves the relative magnitudes of excitatory contributions while
  producing interpretable distributions.

### 2.3 Numerical Stability via Log-Space Prefix Sums

The cumulative decay product $\prod_{j=i+1}^{t-1} d_j[h,d]$ can underflow to zero
for large distances $t - i$, since each $d_j \in (0, 1)$. We compute in log-space
using prefix sums.

**Step 1: Compute log-decay per timestep.**

$$\ell_t[h,d] = \ln\bigl(d_t[h,d]\bigr) = -\exp\bigl(w_t[h,d]\bigr)$$

Note: $\ell_t < 0$ always (decay is always less than 1).

**Step 2: Compute prefix sums of log-decay.**

$$P[0] = 0, \quad P[k] = P[k-1] + \ell_{k-1}[h,d] = \sum_{j=0}^{k-1} \ell_j[h,d]$$

**Step 3: Compute cumulative decay via prefix difference.**

$$\prod_{j=i+1}^{t-1} d_j[h,d] = \exp\bigl(P[t] - P[i+1]\bigr)$$

For $i = t - 1$: $P[t] - P[t] = 0$, so $\exp(0) = 1$ (no decay for the immediately
preceding position, as expected).

**Step 4: Compute per-channel contributions in linear space.**

$$c_{t,i}[h,d] = r_t[h,d] \cdot k_i[h,d] \cdot \exp\bigl(P[t] - P[i+1]\bigr)$$

**Step 5: Sum over channels to get the scalar attention weight.**

$$\alpha_{\text{raw}}(t, i, h) = \sum_{d=1}^{D} c_{t,i}[h,d]$$

This approach avoids underflow while correctly handling the signed $r \cdot k$ products
(the exponentiated prefix difference is always positive; the sign comes from $r \cdot k$).

### 2.4 Properties of the Effective Attention Matrix

The resulting matrix $\alpha \in \mathbb{R}^{H \times T \times T}$ per layer has the
following properties:

| Property | Value |
|----------|-------|
| Shape | $[\text{batch}, H, T, T]$ where $H = 32$, $T = \text{seq\_len}$ |
| Causal | Lower-triangular (position $t$ can only attend to positions $\leq t$) |
| Row sums | Exactly 1.0 (by L1 normalisation) |
| Non-negative | Yes (by ReLU) |
| Decay property | Nearby tokens receive more weight than distant tokens (exponential decay) |

**Validation (GPU, F32):**
- Row sums = 1.0 exactly (layer 0: 337/384 valid rows, layer 23: 235/384)
- Hidden output from `forward_with_effective_attention` is bit-exact with
  `forward_with_cache` (diff = 0.0) — the effective attention computation does
  not alter the forward pass

---

## 3. State Knockout

State knockout is a causal intervention that tests whether a specific token's
contribution to the recurrent state is necessary for the model's output. It is
the RWKV analogue of attention knockout in transformers.

### 3.1 Intervention Design

At a target (marker) position $m$ and specified layers, replace the normal state
update with a **write-suppressed** variant:

$$\text{Normal:} \quad S_m = \text{diag}(d_m) \cdot S_{m-1} + k_m \, v_m^\top$$

$$\boxed{\text{Knockout:} \quad S_m = \text{diag}(d_m) \cdot S_{m-1}}$$

The $k_m \, v_m^\top$ write is suppressed while the decay $\text{diag}(d_m)$ is
preserved. This means:

- The marker token's information is never written to the state
- The forgetting dynamics continue normally — the model still "forgets" older
  information at the same rate
- All subsequent tokens process a state that never saw the marker

**Why preserve the decay?** The alternative ($S_m = S_{m-1}$, no decay) would
artificially preserve old information that would normally be forgotten, introducing
a confound. By keeping the decay, the only difference between baseline and knockout
is the *absence* of the marker's contribution — a clean causal intervention.

### 3.2 Relationship to Transformer Attention Knockout

| Property | Transformer Knockout | RWKV-6 State Knockout |
|----------|---------------------|-----------------------|
| Intervention | Set $A_{t,m} = 0$ (pre-softmax $-\infty$ masking) | Suppress $k_m v_m^\top$ write |
| Granularity | Per-edge $(t, m)$ or all-edge (all $t > m$) | All-edge only (position-based) |
| Effect | Position $t$ cannot read from $m$ | All future positions cannot read from $m$ |
| PLIP equivalent | All-edge knockout (Type 2) | State knockout |

RWKV-6 state knockout is equivalent to the **all-edge knockout** from PLIP's
transformer experiments — making the marker invisible to all future positions. There
is no RWKV equivalent of specific-edge knockout (Type 1), because the recurrent
state is shared across all future positions.

### 3.3 Metric: KL Divergence

The effect of knockout is measured by the KL divergence between the baseline and
knocked-out output distributions:

$$D_{\text{KL}}(P_{\text{baseline}} \| P_{\text{knockout}}) = \sum_{v} P_{\text{baseline}}(v) \cdot \ln \frac{P_{\text{baseline}}(v)}{P_{\text{knockout}}(v)}$$

where $P(v) = \text{softmax}(\text{logits})$ over the vocabulary at the last token
position. Higher KL divergence indicates that the knocked-out token was more important
for the model's prediction.

**First result:** Layer 2, Python mean KL = 0.111%, Rust mean KL = 0.024%,
ratio = 4.6x, p = 0.018 (Welch's t-test).

---

## 4. State Steering

State steering is a dose-response intervention that scales the state write at target
positions rather than suppressing it entirely:

$$\text{Steered:} \quad S_m = \text{diag}(d_m) \cdot S_{m-1} + \lambda \cdot k_m \, v_m^\top$$

where $\lambda \in \mathbb{R}^+$ is the steering scale factor. At $\lambda = 0$, this
reduces to state knockout; at $\lambda = 1$, this is the baseline; at $\lambda > 1$,
the marker's contribution is amplified.

This enables dose-response experiments that measure how the model's output changes
as a function of the marker's state influence, analogous to the attention steering
experiments for transformer backends.

### 4.1 Persistence at Distance

The steered state write $\lambda \cdot k_m \, v_m^\top$ propagates through the
recurrence. At distance $\delta$ from the write, the surviving contribution is:

$$\lambda \cdot k_m \, v_m^\top \cdot \prod_{j=m+1}^{m+\delta} \text{diag}(d_j)$$

This predicts that the marker's influence on generation decays exponentially with
distance, modulated by the data-dependent decay factors $d_j$.

**Empirical validation** (v1.2.0, `state_steering_persistence` example):
- **Distance effect confirmed**: Test syntax rate drops monotonically from 74%
  (86 tokens) to 44% (126 tokens) to 36% (207 tokens), consistent with exponential
  decay (n=30, Rust prompts, temp=0.8).
- **Scale effect null**: Amplifying $\lambda$ from 1.0 to 9.0 has no measurable
  effect on generation (all rates within sampling noise at ~50-53%). The marker
  write at layer 2 is *necessary* for normal predictions (knockout p=0.018) but
  *not sufficient* to steer generation through amplification alone — the bottleneck
  lies elsewhere (later layers, FFN pathway, or language model priors).

---

## 5. State Delta Analysis (Planned)

The following metrics are planned for v1.3.0 and are included here for completeness.
They provide RWKV-native interpretability tools that have no direct transformer
equivalent.

### 5.1 Write Strength

The Frobenius norm of the outer product at position $m$ quantifies how much
information the token writes to the state:

$$\text{WriteStrength}(m) = \|k_m \, v_m^\top\|_F$$

Tokens with high write strength have a large immediate impact on the recurrent state.

### 5.2 Persistence

The persistence metric tracks how much of a token's write survives as the state
evolves through subsequent positions. At distance $\delta$ from the write:

$$\text{Persistence}(m, \delta) = \frac{\bigl\|\text{proj}\bigl(S_{m+\delta},\; \Delta S_m\bigr)\bigr\|_F}{\|\Delta S_m\|_F}$$

where $\Delta S_m = k_m \, v_m^\top$ is the write component and $\text{proj}(S, \Delta)$
is the projection of the full state onto the write direction. Persistence decays
over time due to the forgetting dynamics $\text{diag}(d_t)$ and interference from
subsequent writes.

### 5.3 Channel Selectivity

Spectral analysis of the write component reveals which directions in the state space
are most affected:

$$\text{Spectrum}(\Delta S_m) = \text{SVD}(k_m \, v_m^\top)$$

The singular values indicate whether the write is concentrated in a few channels
(high selectivity) or spread uniformly (low selectivity).

### 5.4 Cross-Validation with Knockout

The persistence metric must be validated against knockout results (Section 3):
tokens with high write strength and high persistence should be the same tokens
whose knockout causes maximal KL divergence. If this correlation holds, persistence
is a lighter-weight proxy for knockout experiments. If it does not, the state
dynamics are more complex than the linear projection model assumes — itself a
scientifically informative result.

---

## 6. Relationship to Prior Work and Novelty

### 6.1 Zimerman, Ali & Wolf (ICLR 2025) — Implicit Attention for Gated-Linear RNNs

Zimerman et al. derive an implicit attention formulation for several gated-linear
RNNs including RWKV, Mamba, and Griffin. Their RWKV formulation (Equations 19-20)
uses a **fixed, scalar** decay parameter $w$:

$$\text{wkv}_t = \frac{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} \otimes v_i + e^{u + k_t} \otimes v_t}{\sum_{i=1}^{t-1} e^{-(t-1-i)w + k_i} + e^{u + k_t}}$$

This corresponds to the **RWKV-4** architecture, where $w$ is a learned parameter
that is the same for all timesteps. Their formulation does **not** handle RWKV-6's
data-dependent decay, where $w_t$ varies per timestep (generated via LoRA from the
input) and is vector-valued (one value per head channel).

Although the paper mentions RWKV-6 in its related work section, the mathematical
derivation in Section 3.3 uses the static-decay formulation throughout. The v2
revision (October 2024) does not extend the derivation to the data-dependent case.

### 6.2 Endy et al. (ACL 2025) — Mamba Knockout

Endy et al. introduce "Mamba Knockout", adapting attention knockout to Mamba-1/2
state-space models. Their technique operates on the kernel matrix
$\mathbf{M}_{i,j} = Q_i \cdot H_{i,j} \cdot K_j$, setting specific entries to zero
to block token-to-token information flow. They also introduce "feature knockout",
which selectively ablates context-dependent vs. context-independent features based
on decay norms.

Their work applies to **Mamba only**, not RWKV. The intervention targets the
factored kernel matrix rather than the recurrent state directly. PLIP-rs's state
knockout for RWKV-6 (suppressing the $k_m v_m^\top$ state write while preserving
decay) is a distinct mechanism adapted to RWKV-6's architecture, though it shares
the same conceptual goal: testing which tokens are causally important by removing
their contribution to the model's state.

### 6.3 Paulo, Marshall & Belrose (AAAI 2025) — Interpretability Transfer

Paulo et al. investigate whether transformer interpretability methods (contrastive
activation addition, tuned lens, eliciting latent knowledge) transfer to RNNs
including Mamba and RWKV. They find that most methods transfer effectively and can
sometimes be improved by leveraging the RNN's compressed state.

Their work establishes that MI methods can cross the transformer/RNN boundary but
does not derive effective attention matrices or perform state-level knockout
experiments.

### 6.4 What PLIP-rs Contributes

**Extensions of prior work:**

1. **Effective attention for RWKV-6 with data-dependent, vector-valued decay.**
   Zimerman et al. derive implicit attention for RWKV-4's fixed scalar decay.
   PLIP-rs extends this to RWKV-6, where the decay $w_t$ is generated via LoRA
   at each timestep and is a vector of dimension $D$ per head. The derivation
   (Section 2.1) accounts for per-channel cumulative decay products and aggregation
   via dot product — a non-trivial extension since the decay is no longer a simple
   geometric series.

2. **State knockout for RWKV-6.** Endy et al. perform knockout on Mamba's
   kernel matrix. PLIP-rs performs knockout on RWKV-6's recurrent state by
   suppressing the $k_m v_m^\top$ write while preserving decay dynamics
   (Section 3.1). This is the first published state knockout result for RWKV-6
   (p = 0.018, layer 2).

**Practical contributions:**

3. **ReLU + L1 normalisation** for handling signed effective attention weights.
   The signed $r \cdot k$ products in RWKV-6 require a non-standard normalisation;
   PLIP-rs uses ReLU to focus on excitatory contributions followed by L1
   normalisation to produce valid distributions (Section 2.2).

4. **Log-space prefix sums** for numerically stable cumulative decay computation
   over long sequences (Section 2.3).

5. **Cross-paradigm comparison** of transformer attention patterns and RWKV-6
   effective attention / state knockout within a unified experimental framework,
   using the same corpus, the same statistical tests, and the same metrics.

**Not claimed as novel:**

- The *concept* of implicit/effective attention for gated-linear RNNs (Zimerman
  et al., ICLR 2025).
- The *concept* of state-level interventions in sequence models (Endy et al.,
  ACL 2025).
- The general transferability of MI methods from transformers to RNNs (Paulo
  et al., AAAI 2025).

---

## 7. Notation Summary

| Symbol | Meaning | Shape |
|--------|---------|-------|
| $T$ | Sequence length | scalar |
| $H$ | Number of heads (32 for 1B6) | scalar |
| $D$ | Head dimension (64 for 1B6) | scalar |
| $d$ | Model hidden dimension ($H \cdot D = 2048$) | scalar |
| $x_t$ | Hidden state at position $t$ | $[d]$ |
| $\mu_x$ | Pre-mixing coefficient (`time_maa_x`) | $[d]$ |
| $\bar{x}_t$ | Pre-mixed LoRA input: $x_t + (x_{t-1} - x_t) \odot \mu_x$ | $[d]$ |
| $m^{(c)}_t$ | Data-dependent mixing correction from LoRA | $[d]$ |
| $\mu^{(c)}_0$ | Per-component learned mixing bias | $[d]$ |
| $\tilde{x}^{(c)}_t$ | Mixed input for component $c$ | $[d]$ |
| $r_t, k_t, v_t$ | Receptance, key, value projections | $[H, D]$ |
| $g_t$ | Gate projection | $[H, D]$ |
| $w_t$ | Data-dependent time decay (pre-exp) | $[H, D]$ |
| $d_t$ | Decay factor: $\exp(-\exp(w_t))$ | $[H, D]$ |
| $u$ | Per-head current-position bonus (`time_faaaa`) | $[H, D]$ |
| $S_t$ | Recurrent state at position $t$ | $[H, D, D]$ |
| $\alpha_{\text{raw}}(t,i,h)$ | Raw effective attention weight | scalar |
| $\alpha(t,i,h)$ | Normalised effective attention weight | scalar |
| $P[k]$ | Log-decay prefix sum at position $k$ | $[H, D]$ |
| $\ell_t$ | Log-decay at position $t$: $-\exp(w_t)$ | $[H, D]$ |
| $\lambda$ | Steering scale factor | scalar |

---

## 8. References

### RWKV Architecture

1. **Peng, B., Alcaide, E., Anthony, Q., et al.** (2023).
   "RWKV: Reinventing RNNs for the Transformer Era."
   *EMNLP 2023 Findings.*
   [arXiv:2305.13048](https://arxiv.org/abs/2305.13048)
   — Original RWKV (v4) architecture paper.

2. **Peng, B., Goldstein, D., Anthony, Q., et al.** (2024).
   "Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence."
   *EMNLP 2024 Findings.*
   [arXiv:2404.05892](https://arxiv.org/abs/2404.05892)
   — Introduces RWKV-5 (Eagle) and RWKV-6 (Finch); data-dependent decay,
   matrix-valued states, LoRA-based mixing.

3. **Peng, B., Goldstein, D., Anthony, Q., et al.** (2025).
   "RWKV-7 'Goose' with Expressive Dynamic State Evolution."
   [arXiv:2503.14456](https://arxiv.org/abs/2503.14456)
   — Successor architecture with generalised delta rule. Not implemented in PLIP-rs.

4. **Duan, H., Zhu, Z., Zhao, S., et al.** (2025).
   "A Survey of RWKV."
   [arXiv:2412.14847](https://arxiv.org/abs/2412.14847)
   — Comprehensive survey covering RWKV-4 through RWKV-7, including architectural
   evolution, training, and applications.

### Interpretability for Gated-Linear RNNs

5. **Zimerman, I., Ali, A., & Wolf, L.** (2025).
   "Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation."
   *ICLR 2025.*
   [arXiv:2405.16504](https://arxiv.org/abs/2405.16504) |
   [OpenReview](https://openreview.net/forum?id=wnT8bfJCDx) |
   [Code](https://github.com/Itamarzimm/UnifiedImplicitAttnRepr)
   — Derives implicit attention matrices for RWKV (static decay / RWKV-4 formulation),
   Mamba, and Griffin. Provides explainability framework. Does not cover RWKV-6's
   data-dependent decay.

6. **Endy, N., Ben-Ari, R., & Ravfogel, S.** (2025).
   "Mamba Knockout for Unraveling Factual Information Flow."
   *ACL 2025.*
   [arXiv:2505.24244](https://arxiv.org/abs/2505.24244) |
   [ACL Anthology](https://aclanthology.org/2025.acl-long.1143/)
   — Adapts attention knockout to Mamba-1/2 via kernel matrix zeroing and introduces
   feature knockout. Applies to Mamba only, not RWKV.

7. **Paulo, G., Marshall, T., & Belrose, N.** (2025).
   "Do Transformer Interpretability Methods Transfer to RNNs?"
   *AAAI 2025.*
   [arXiv:2404.05971](https://arxiv.org/abs/2404.05971) |
   [AAAI Proceedings](https://ojs.aaai.org/index.php/AAAI/article/view/34969)
   — Tests contrastive activation addition, tuned lens, and eliciting latent
   knowledge on Mamba and RWKV. Finds most methods transfer successfully.

### PLIP-rs

8. **Jacopin, E. & Claude.** (2026).
   "PLIP-rs: Programming Language Internal Probing in Rust."
   [GitHub](https://github.com/PCfVW/plip-rs)
   — This project. Implements effective attention (Section 2), state knockout
   (Section 3), and state steering (Section 4) for RWKV-6.
